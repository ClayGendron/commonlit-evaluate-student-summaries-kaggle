{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import textstat\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from language_tool_python import LanguageTool\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import RobertaModel, RobertaTokenizer, RobertaForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = Path(os.getcwd())\n",
    "kaggle_dir = Path('/kaggle/input')\n",
    "\n",
    "notebook_dir = local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(notebook_dir / 'commonlit-evaluate-student-summaries' / 'summaries_train.csv')\n",
    "train_prompts_df = pd.read_csv(notebook_dir / 'commonlit-evaluate-student-summaries' / 'prompts_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Conent Model (RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(prompt, text, model, tokenizer, max_length=128):\n",
    "    \"\"\"Summarize long text with prompt using T5 model\"\"\"\n",
    "    input_text = 'summarize: ' + prompt + text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(input_ids, max_length=max_length, length_penalty=5.0, num_beams=5, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def split_data(df, prompt_title):\n",
    "    \"\"\"Split data into training and validation sets based on prompt_title\"\"\"\n",
    "    training_df = df[df['prompt_title'] != prompt_title]\n",
    "    validation_df = df[train_df['prompt_title'] == prompt_title]\n",
    "    return training_df, validation_df\n",
    "\n",
    "def prepare_data(df, tokenizer, batch_size, shuffle, target):\n",
    "    \"\"\"Prepare data into DataLoader for training and validation\"\"\"\n",
    "    prompts = df['prompt_text_summary'].to_list()\n",
    "    responses = df['text'].to_list()\n",
    "    scores = df[target].to_list()\n",
    "    \n",
    "    encodings = tokenizer(prompts, responses, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    scores_tensor = torch.tensor(scores).unsqueeze(-1).float()\n",
    "    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], scores_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return loader\n",
    "\n",
    "def compute_model_loss(model, data_loader, device):\n",
    "    \"\"\"Compute MSE loss of model on data_loader\"\"\"\n",
    "    model.eval()  # set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    mse = total_loss / len(data_loader)\n",
    "    \n",
    "    model.train()  # set the model back to training mode\n",
    "    return mse\n",
    "\n",
    "def train_roberta(model_path, training_loader, validation_loader, device, lr=3e-5, epochs=100):\n",
    "    \"\"\"Train RoBERTa model with automatic saving of best model and early stopping\"\"\"\n",
    "    print(f'Loading RoBERTa model from {model_path}...')\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=1)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    initial_val_loss = compute_model_loss(model, validation_loader, device)\n",
    "\n",
    "    print('Initial validation loss:', initial_val_loss, '\\n')\n",
    "    print('Training model...')\n",
    "    \n",
    "    epochs_without_improvement = 0\n",
    "    no_improvement_since_reset = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for batch in training_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = total_loss / len(training_loader)\n",
    "        val_loss = compute_model_loss(model, validation_loader, device)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs} - Training Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < initial_val_loss:\n",
    "            epochs_without_improvement = 0\n",
    "            no_improvement_since_reset = 0\n",
    "            initial_val_loss = val_loss\n",
    "            print('Improvement in validation loss. Saving model.')\n",
    "            model.save_pretrained(model_path)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement == 5:\n",
    "            no_improvement_since_reset += 1\n",
    "\n",
    "            if no_improvement_since_reset == 2:\n",
    "                print('No more improvement in validation loss. Stopping training.')\n",
    "                break\n",
    "            else:\n",
    "                print('No improvement in validation loss for 5 epochs.')\n",
    "                print('Resetting model to last saved state.')\n",
    "                no_improvement_since_reset += 1\n",
    "                model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=1)\n",
    "                model.to(device)\n",
    "\n",
    "    print('Training complete. Final validation MSE:', initial_val_loss, '\\n')\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5 for Prompt Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0d56fa49ab435883d7b78668413403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\c.gendron1\\AppData\\Local\\miniconda3\\envs\\playground\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\c.gendron1\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b724fcf7d2ee4b1e969a941a2dfe6c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5265e654af044794853ecf7cdb170ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\c.gendron1\\Git\\commonlit-evaluate-student-summaries-kaggle\\model_training.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/c.gendron1/Git/commonlit-evaluate-student-summaries-kaggle/model_training.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m t5_model \u001b[39m=\u001b[39m T5ForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mt5-base\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/c.gendron1/Git/commonlit-evaluate-student-summaries-kaggle/model_training.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m t5_tokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mt5-base\u001b[39m\u001b[39m'\u001b[39m, model_max_length \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\c.gendron1\\AppData\\Local\\miniconda3\\envs\\playground\\lib\\site-packages\\transformers\\utils\\import_utils.py:1222\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_from_config\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1221\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1222\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[1;32mc:\\Users\\c.gendron1\\AppData\\Local\\miniconda3\\envs\\playground\\lib\\site-packages\\transformers\\utils\\import_utils.py:1210\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1208\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[0;32m   1209\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m-> 1210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompts_df['prompt_text_summary'] = np.zeros(len(train_prompts_df), dtype=object)\n",
    "for i, row in train_prompts_df.iterrows():\n",
    "    prompt_summary = summarize_text(row['prompt_question'], row['prompt_text'], t5_model, t5_tokenizer)\n",
    "    train_prompts_df.loc[i, 'prompt_text_summary'] = prompt_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = notebook_dir / 'roberta_content_scorer'\n",
    "device = torch.device('mps') if torch.mps.is_available() else torch.device('cpu')\n",
    "\n",
    "roberta_content_model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=1)\n",
    "roberta_tozenizer = RobertaTokenizer.from_pretrained('roberta-base', model_max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(train_prompts_df, on='prompt_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, validation_df = split_data(train_df, 'On Tragedy')\n",
    "\n",
    "training_loader = prepare_data(\n",
    "    training_df, \n",
    "    roberta_tozenizer, \n",
    "    batch_size=8, \n",
    "    shuffle=True, \n",
    "    target='content'\n",
    ")\n",
    "\n",
    "validation_loader = prepare_data(\n",
    "    validation_df, \n",
    "    roberta_tozenizer, \n",
    "    batch_size=8, \n",
    "    shuffle=False, \n",
    "    target='content'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_content_model = train_roberta(model_path, training_loader, validation_loader, device, lr=3e-5, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
